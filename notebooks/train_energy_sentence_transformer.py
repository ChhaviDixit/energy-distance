# -*- coding: utf-8 -*-
"""train_energy_sentence_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q0BRkgQeti5zKJFP6w2kVxeWyhvYyRb2
"""

import sys
import os

"""Install basic python requirements"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -r ../requirements.txt --quiet

"""Clone version of sentence-transformers that includes energy distance implementation."""

#!git clone https://github.com/gnatesan/sentence-transformers-energydistance

"""Install custom sentence-transformers using `pip`"""

# Commented out IPython magic to ensure Python compatibility.
# %cd sentence-transformers-energydistance
# %pip install --upgrade pip --quiet
# %pip install . --quiet
# %cd ..

sys.path.append(f'{os.getcwd()}/sentence_transformers_energydistance')

"""Ensure that you're using a GPU with enough available memory."""

import torch

"""def check_available_gpus():
    gpu_stats = []
    for i in range(torch.cuda.device_count()):
        device = torch.device('cuda')	
        total_memory = torch.cuda.get_device_properties(i).total_memory
        allocated_memory = torch.cuda.memory_allocated(i)
        free_memory = total_memory - allocated_memory
        gpu_stats.append((i, free_memory))
    # Sort GPUs by the most free memory
    gpu_stats.sort(key=lambda x: x[1], reverse=True)
    return gpu_stats

print(check_available_gpus())"""

device = torch.device("cuda")
print("Using device:", device)

from sentence_transformers import SentenceTransformer, models

"""for gpu_id, _ in check_available_gpus():
    try:
        ## Step 1: use an existing language model
        #word_embedding_model = models.Transformer('distilroberta-base')

        ## Step 2: use a pool function over the token embeddings
        #pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())

        ## Join steps 1 and 2 using the modules argument
        model = SentenceTransformer('all-MiniLM-L6-v2')
    except RuntimeError as e:
        if 'out of memory' in str(e):
            print(f"GPU {gpu_id} ran out of memory, trying next available GPU.")
            torch.cuda.empty_cache()  # Clear memory cache
            continue
        else:
            raise e"""

model = SentenceTransformer('all-MiniLM-L6-v2')


from datasets import load_dataset

dataset_id = "embedding-data/QQP_triplets"
dataset = load_dataset(dataset_id)

print(f"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.")
print(f"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.")
print(f"- Examples look like this: {dataset['train'][0]}")

from sentence_transformers import InputExample

train_examples = []
train_data = dataset['train']['set']
# Use all training data
n_examples = dataset['train'].num_rows // 2

for i in range(n_examples):
  example = train_data[i]
  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))

print(f"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.")

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

from sentence_transformers import losses

train_loss = losses.TripletLoss(model=model)

num_epochs = 10

warmup_steps = int(len(train_dataloader) * num_epochs * 0.8) #80% of train data

model.fit(train_objectives=[(train_dataloader, train_loss)],
          epochs=num_epochs,
          warmup_steps=warmup_steps)

"""Save output to `/models`"""

model_name = 'ed-all-MiniLM-L6-v2_TripletLoss_4'
os.makedirs(f'{os.getcwd()}/../models', exist_ok=True)
model_path = f'{os.getcwd()}/../models/{model_name}'
model.save(model_path)
